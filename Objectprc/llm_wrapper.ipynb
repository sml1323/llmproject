{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **LLM Wrapper 클래스 이해 및 구현 학습**\n",
    "- LLM 챗봇 연습에서 공통적인 부분을 리팩터링 하기 위한 학습\n",
    "\n",
    "1.  **추상 클래스 (`ABC`와 `@abstractmethod`) 개념 이해**\n",
    "\n",
    "\n",
    "2.  **`LLM_Wrapper` 클래스 분석 및 구현**\n",
    "\n",
    "\n",
    "3.  **`GPT_Wrapper` 클래스 분석 및 구현**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 추상 클래스 - `ABC`, `@abstractmethod`\n",
    "\n",
    "추상 클래스는 따라야 할 공통적인 메서드만 정의하고, 구체적인 내용은 비워두는 **레시피의 틀**과 같다.\n",
    "\n",
    "따라서 추상 클래스를 상속받는 자식 클래스는, 자신만의 방식으로 비워진 부분들을 채워 넣어야 함\n",
    "\n",
    "파이썬에서는 `abc` 모듈의 `ABC` 와 `@abstractmethod` 데코레이터를 사용해서 이런 추상 클래스를 만들 수 있다.\n",
    "- `ABC`: 클래스를 추상 클래스로 만들어 준다. 이 클래스 자체로는 인스턴스를 만들 수 없고, 반드시 상속받아서 사용\n",
    "- `abstractmethod`: 이 데코레이터가 붙은 메서드는 자식 클래스에서 반드시 구현해야 하는 `추상 메서드`가 된다. -> 오류를 발생시켜 강제함\n",
    "\n",
    "여러가지 **`LLM`** 들의 응답을 받기 위해서는 세부적인 내용은 다르지만, 메세지 보내기, 모델 정의, 응답 받기 등의 큰 틀은 같다. \n",
    "- `LLM_Wrappr` 라는 추상 클래스로 공통 기능들을 정의하면, 코드 구성할때 더 좋고 쉬움\n",
    "\n",
    "구현 실습 1\n",
    "\n",
    "1.  Vehicle 추상 클래스와 start_engine() 추상 메서드를 정의\n",
    "2. Car 클래스를 정의하고 Vehicle을 상속받아 start_engine()을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car start engine!\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Vehicle(ABC):\n",
    "    @abstractmethod\n",
    "    def start_engine(self):\n",
    "        pass\n",
    "\n",
    "class Car(Vehicle):\n",
    "    def start_engine(self):\n",
    "        print(\"Car start engine!\")\n",
    "\n",
    "mycar = Car()\n",
    "mycar.start_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "구현실습 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc 모듈에서 필요한 것을 임포트하세요.\n",
    "\n",
    "# 여기에 LLM_Wrapper 추상 클래스를 정의하세요.\n",
    "# getResult 추상 메서드만 포함합니다.\n",
    "\n",
    "# 힌트: 추상 클래스를 만들기 위해 ABC를 상속받아야 합니다.\n",
    "# 힌트: getResult 메서드는 @abstractmethod 데코레이터가 필요합니다.\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "class LLM_Wrapper(ABC):\n",
    "    @abstractmethod\n",
    "    def get_result(self):\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구현 (기본 속성 및 __init__ 추가):\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예상된 에러: Can't instantiate abstract class LLM_Wrapper with abstract method get_result\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 위에서 만든 LLM_Wrapper 클래스에 다음 속성들을 추가하고, 이 속성들을 초기화하는 __init__ 메서드를 구현해 보세요. API 키 로딩 부분은 아직 넣지 않아도 됩니다.\n",
    "\n",
    "# system_prompt: str\n",
    "\n",
    "# user_prompt: str\n",
    "\n",
    "# temperature: float = 0.5\n",
    "\n",
    "# __msg: MutableSequence[MessageEntry] (여기서 MessageEntry와 MutableSequence 임포트는 필요합니다.)\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import MutableSequence\n",
    "from typing import TypedDict\n",
    "\n",
    "class LLM_Wrapper(ABC):\n",
    "\n",
    "    MessageEntry = TypedDict('MessageEntry', {\"role\" : str, \"content\" : str})\n",
    "    system_prompt : str\n",
    "    user_prompt : str \n",
    "    temperature : float = 0.5\n",
    "\n",
    "    def __init__(self, system_prompt, user_prompt, temperature):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n",
    "        self.temperature = temperature\n",
    "        __msg : MutableSequence[MessageEntry]\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_result(self):\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    wrapper = LLM_Wrapper(\"시스템 프롬프트\", \"사용자 프롬프트\")\n",
    "except TypeError as e:\n",
    "    print(f\"예상된 에러: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구현 - 메세지 관리 메서드 추가\n",
    "- messageSet(self, message: MutableSequence[MessageEntry])\n",
    "\n",
    "- messageAppend(self, role: str, content: str)\n",
    "\n",
    "- messageGet(self) -> MutableSequence[MessageEntry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import MutableSequence\n",
    "from typing import TypedDict\n",
    "\n",
    "class LLM_Wrapper(ABC):\n",
    "\n",
    "    MessageEntry = TypedDict('MessageEntry', {\"role\" : str, \"content\" : str})\n",
    "    system_prompt : str\n",
    "    user_prompt : str \n",
    "    temperature : float = 0.5\n",
    "    __msg : MutableSequence[MessageEntry]\n",
    "\n",
    "    def __init__(self, system_promt, user_promt, temperature):\n",
    "        self.system_promt = system_promt\n",
    "        self.user_promt = user_promt\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def messageSet(self, message: MutableSequence[MessageEntry]):\n",
    "        self.__msg = message\n",
    "\n",
    "    def messageAppend(self, role: str, content: str):\n",
    "        self.__msg.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def messageGet(self) -> MutableSequence[MessageEntry]:\n",
    "        return self.__msg\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_result(self):\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    wrapper = LLM_Wrapper(\"시스템 프롬프트\", \"사용자 프롬프트\")\n",
    "except TypeError as e:\n",
    "    print(f\"예상된 에러: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM_WRAPPER를 상속받는 자식 클래스 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_Wrapper 클래스가 있는 파일에서 임포트했다고 가정합니다.\n",
    "# from your_module import LLM_Wrapper # 실제 프로젝트에서는 이렇게 임포트합니다.\n",
    "\n",
    "# 임시로 LLM_Wrapper 정의 (실제로는 import 해서 사용)\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import MutableSequence\n",
    "from typing import TypedDict\n",
    "class LLM_Wrapper(ABC):\n",
    "    MessageEntry = TypedDict('MessageEntry', {\"role\" : str, \"content\" : str})\n",
    "    system_prompt : str\n",
    "    user_prompt : str \n",
    "    temperature : float = 0.5\n",
    "    __msg : MutableSequence[MessageEntry]\n",
    "    def __init__(self, system_prompt: str, user_prompt: str, temperature: float = 0.5, env_apikey_var:str=None):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n",
    "        self.temperature = temperature\n",
    "        self.__msg = []\n",
    "        # API 키 로딩 로직은 여기서는 생략했지만, 실제 LLM_Wrapper에는 포함되어야 합니다.\n",
    "    def messageSet(self, message: MutableSequence[MessageEntry]): self.__msg = message\n",
    "    def messageAppend(self, role: str, content: str): self.__msg.append({\"role\": role, \"content\": content})\n",
    "    def messageGet(self) -> MutableSequence[MessageEntry]: return self.__msg\n",
    "    @abstractmethod\n",
    "    def getResult(self): pass\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# LLM_Wrapper 정의는 퀴즈 1과 동일하게 있다고 가정합니다.\n",
    "\n",
    "# 여기에 GPT_Wrapper 클래스를 완성하세요.\n",
    "# class GPT_Wrapper(LLM_Wrapper):\n",
    "#     MODEL:str = 'gpt-4o-mini'\n",
    "#     llm:OpenAI # 타입 힌트\n",
    "\n",
    "#     def __init__(self, system_prompt:str, user_prompt:str):\n",
    "#         # super().__init__ 호출\n",
    "#         # self.llm 초기화\n",
    "#         # super().messageSet 호출\n",
    "#         pass \n",
    "\n",
    "#     def getResult(self):\n",
    "#         # 간단한 문자열을 반환하도록 구현\n",
    "#         pass\n",
    "\n",
    "# 테스트 코드 (객체 생성 및 getResult 호출)\n",
    "# gpt_instance = GPT_Wrapper(\"당신은 친절한 챗봇입니다.\", \"안녕하세요!\")\n",
    "# print(gpt_instance.getResult())\n",
    "\n",
    "# ----------------\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# LLM_Wrapper 정의는 퀴즈 1과 동일하게 있다고 가정합니다.\n",
    "\n",
    "# 여기에 GPT_Wrapper 클래스를 완성하세요.\n",
    "class GPT_Wrapper(LLM_Wrapper):\n",
    "    MODEL:str = 'gpt-4o-mini'\n",
    "    llm:OpenAI # 타입 힌트\n",
    "\n",
    "    def __init__(self, system_prompt:str, user_prompt:str):\n",
    "        super().__init__(system_prompt, user_prompt)\n",
    "        self.llm = OpenAI()\n",
    "        super().messageSet([\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt}\n",
    "        ])\n",
    "\n",
    "    def getResult(self):\n",
    "        # 간단한 문자열을 반환하도록 구현\n",
    "        return \"GPT\"\n",
    "\n",
    "# 테스트 코드 (객체 생성 및 getResult 호출)\n",
    "gpt_instance = GPT_Wrapper(\"당신은 친절한 챗봇입니다.\", \"안녕하세요!\")\n",
    "print(gpt_instance.getResult())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저는 다양한 주제에 대해 대화하고, 정보를 제공하는 것을 좋아해요. 여러분의 질문에 답하거나, 도움이 될 수 있는 내용을 찾는 것이 제 취미라고 할 수 있습니다. 여러분의 취미는 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "# 기존 LLM_Wrapper와 GPT_Wrapper 클래스 코드는 그대로 사용한다고 가정합니다.\n",
    "from openai import OpenAI\n",
    "# import json # JSON 응답 처리를 위해 필요할 수 있지만, 이번 퀴즈에서는 제외해도 됩니다.\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# GPT_Wrapper 클래스의 getResult 메서드만 수정\n",
    "class GPT_Wrapper(LLM_Wrapper):\n",
    "    MODEL:str = 'gpt-4o-mini'\n",
    "    llm:OpenAI\n",
    "\n",
    "    def __init__(self, system_prompt:str, user_prompt:str):\n",
    "        super().__init__(system_prompt, user_prompt)\n",
    "        self.llm = OpenAI()\n",
    "        super().messageSet([\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt}\n",
    "        ])\n",
    "\n",
    "    def getResult(self):\n",
    "        # 여기에 OpenAI API 호출 로직을 완성하세요.\n",
    "        # model은 self.MODEL, messages는 super().messageGet(), temperature는 self.temperature를 사용하세요.\n",
    "        # 응답의 첫 번째 메시지 content를 반환합니다.\n",
    "        response = self.llm.chat.completions.create(\n",
    "            model=self.MODEL,\n",
    "            messages=super().messageGet(), # 부모 클래스의 메시지 가져오기\n",
    "            temperature=self.temperature   # 부모 클래스의 temperature 사용\n",
    "        )\n",
    "        return response.choices[0].message.content # 응답 내용 반환\n",
    "\n",
    "# 테스트 코드 (실제로 GPT API 호출)\n",
    "# .env 파일에 OPENAI_API_KEY가 설정되어 있어야 합니다!\n",
    "chat_gpt = GPT_Wrapper(\"당신은 친절한 친구입니다. 한국어로 대답해주세요.\", \"취미가 무엇인가요?\")\n",
    "print(chat_gpt.getResult())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
