{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!\n",
    "\n",
    "---\n",
    "\n",
    "gpt-4o-mini, llama3.2 를 이용한 간단한 프로젝트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key가 괜찮아 보여요\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if api_key and api_key.startswith('sk-') and len(api_key) > 10:\n",
    "    print(\"API key가 괜찮아 보여요\")\n",
    "else:\n",
    "    print(\"API key에 문제가 있는거 같은데, 확인해봐야 할듯\")\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "\"\"\"\n",
    "\n",
    "default_question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bee4c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutor_system_prompt = \"\"\"You are a helpful coding tutor. \n",
    "You have a deep understanding of Python and can explain code precisely and clearly.\n",
    "This is for a student who is intermediate in Python, and for whom Korean is their first language.\n",
    "So please answer in Korean.\n",
    "And to ensure student's knowledge, please ask a follw-up question at the end of your answer.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "이 코드는 파이썬의 `yield from` 구문과 집합 표현식(comprehension)을 사용하여 특정 책들의 저자(author) 정보를 생성(generator)하는 역할을 합니다. 자세히 설명하자면:\n",
       "\n",
       "1. **집합 표현식**:\n",
       "   ```python\n",
       "   {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "   ```\n",
       "   이 부분은 `books`라는 리스트(또는 이터러블)에 있는 각 `book` 딕셔너리에서 저자 정보를 추출하고 있습니다. `if book.get(\"author\")` 조건을 사용하여 저자 정보가 존재하는 경우에만 저자 이름을 포함시킵니다. 이렇게 생성된 값들은 중복을 허용하지 않는 집합(set) 형태가 됩니다.\n",
       "\n",
       "2. **`yield from` 구문**:\n",
       "   ```python\n",
       "   yield from { ... }\n",
       "   ```\n",
       "   `yield from`은 특정 이터러블(이 경우에는 집합)을 반환하는 역할을 합니다. 이를 통해 이터레이터를 반환하고, 나중에 값을 일일이 리턴하는 대신 더 간단하게 값을 생성할 수 있습니다.\n",
       "\n",
       "전체적으로 이 코드는 `books` 리스트에서 저자 정보를 추출하여 중복 없이 이터레이터로 생성하는 함수의 일부일 수 있습니다. 이 코드를 사용하는 함수는 호출될 때마다 저자 이름을 하나씩 반환할 수 있게 됩니다.\n",
       "\n",
       "이해를 돕기 위해, 예를 들어 `books`가 다음과 같은 내용이라고 가정해 봅시다:\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"책1\", \"author\": \"저자1\"},\n",
       "    {\"title\": \"책2\", \"author\": \"저자2\"},\n",
       "    {\"title\": \"책3\", \"author\": None},\n",
       "    {\"title\": \"책4\", \"author\": \"저자1\"}\n",
       "]\n",
       "```\n",
       "위의 코드에서는 `저자1`, `저자2`라는 값이 생성되며, `저자1`은 중복이 제거된 상태로 하나만 반환됩니다.\n",
       "\n",
       "질문: 이 코드에서 `yield from`을 사용하지 않고, 일반 `for` 루프를 사용했다고 가정하면, 코드는 어떻게 달라질까요?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "question = input('코드에 관한 질문을 입력하세요: ')\n",
    "def get_answer(model=MODEL_GPT, question : str = default_question):\n",
    "    response = openai.chat.completions.create(\n",
    "        model = MODEL_GPT,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": tutor_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))\n",
    "\n",
    "get_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 코드는 여러 Books에 대한 정보를 한 sefer에 읽기 더efficient한 방식을 제공한다.\n",
      "\n",
      "`yield from` keyword는 generator function을return하는 function을return할 때 사용한다. \n",
      "\n",
      "이 코드에서는 `books` variable가 List로 구성된 BOOK informações를 담고 있는 list를 제외할 수 있다. \n",
      "\n",
      "만약 `book.get(\"author\")`가 None일 경우, 이 코드에서는 해당 book의 정보를 읽지 않는다.\n",
      "\n",
      "만약 `yield from`를 사용했다면, 이 코드는 Books information을 한 sefer에 읽기 더efficient한 방식을 제공한다.\n",
      "\n",
      "예를 들어, `books = [{\"title\": \"Book1\", \"author\": \"AuthorA\"}, {\"title\": \"Book2\", \"author\": \"None\"}]`\n",
      "\n",
      "이 경우, `yield from {book.get(\"author\") for book in books if book.get(\"author\")}`는 `[\"AuthorA\", None\"]`를 yield한다.\n",
      "\n",
      "반면, `for book in books: print(book.get(\"author\"))`는 두 번의 읽기를 necessitates한다.\n",
      "\n",
      "```python\n",
      "import re\n",
      "\n",
      "books = [{\"title\": \"Book1\", \"author\": \"AuthorA\"}, {\"title\": \"Book2\", \"author\": \"None\"}]\n",
      "\n",
      "# yield from :效率가 더 good하다 \n",
      "for info in yield_from ({book.get(\"author\") for book in books if book.get(\"author\")}):\n",
      "    print(info)\n",
      "\n",
      "# not yield_from:\n",
      "for book in books: \n",
      "  for author in book.get('author',''): \n",
      "     print(author)\n",
      "```\n",
      "\n",
      "아래에 두가지를 asking하는 question이 있다.\n",
      "\n",
      "1. yield from expression에 대해 further explanation을้องการ?\n",
      "2. `yield from`를 사용할 때, side effect behavior을 control하지 difficult할 수 있다. \n",
      "\n",
      "이러한 side effect behavior에 대해 어떻게 behave 할지 explain한다는 thing을 hỏi하고 싶은가?\n"
     ]
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "def get_answer_llama(question: str = default_question):\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": tutor_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"\"\"\n",
    "                    Please explain what this code does and why:\n",
    "                    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "                 \n",
    "\"\"\"},\n",
    "\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "get_answer_llama(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1c3d9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "- llama의 답변이 정말 이상한데, 맥북 m1에서도 돌아가는 모델의 한계인거같다.\n",
    "- 한국어 부분을 뺀 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1191c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is using Python's generator syntax to extract the \"author\" values from a list of dictionaries (`books`). Here's how it works:\n",
      "\n",
      "1. `yield from`: This keyword starts a new generator. It allows you to delegate the generation of values to another iterable (in this case, a dictionary comprehension).\n",
      "2. `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a dictionary comprehension that iterates over the list of dictionaries (`books`). For each dictionary, it attempts to get the value of the key `\"author\"`. If the key exists, its corresponding value is included in the resulting iterator. The `if` condition filters out dictionaries where `\"author\"` is not present.\n",
      "\n",
      "So, when we combine these two parts with `yield from`, we create a generator that:\n",
      "\n",
      "- Yields each `\"author\"` value found in the list of books.\n",
      "- Stops iterating over the list of books once it's exhausted, as generators are lazily evaluated.\n",
      "\n",
      "In essence, this code returns an iterator that produces author names one by one, without loading all of them into memory at once, which makes it memory-efficient for handling large datasets.\n",
      "\n",
      "Here's a follow-up question:\n",
      "\n",
      "What would you like to do with the yielded \"author\" values, e.g., store them in a list, use them as input for another function, or something else?\n"
     ]
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "def get_answer_llama(question: str = default_question):\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"You are a helpful coding tutor. \n",
    "You have a deep understanding of Python and can explain code precisely and clearly.\n",
    "This is for a student who is intermediate in Python.\n",
    "To ensure student's knowledge, please ask a follw-up question at the end of your answer.\n",
    "\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": \"\"\"\n",
    "                    Please explain what this code does and why:\n",
    "                    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "                 \n",
    "\"\"\"},\n",
    "\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "get_answer_llama(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499a41c",
   "metadata": {},
   "source": [
    "- 영어 답변은 괜찮아보인다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eef0dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
